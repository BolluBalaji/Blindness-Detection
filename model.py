# -*- coding: utf-8 -*-
"""Sgmoid1611_Home_Challenge.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ijvADwhvpxm7Q96T6oEfaGjdVkPbE4UC
"""

from google.colab import drive
drive.mount('/content/drive')

import torch

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image

train = pd.read_csv('/content/drive/My Drive/Sgmoid1611/train.csv')
print(train.shape)
train.head()

# !unzip /content/drive/My\ Drive/Sgmoid1611/train_images.zip

import glob
total_images=0
train_images = []
train_images +=  glob.glob('/content/drive/My Drive/sgmoid_train_imgs/*.png')
total_images = len(train_images)
print(total_images)

# import shutil
# import os
# destination = "/content/drive/My Drive/sgmoid_train_imgs/"
# for i in train_images:
#   path = destination + i
#   dest = shutil.copyfile(i,path)

train_images[0]

# Image.open('/content/drive/My Drive/sgmoid_train_imgs/2d3f4094c08a.png')

# Image.open(train_images[0])
path = '/content/drive/My Drive/sgmoid_train_imgs/'
path + train['id_code'][0] + ".png"

# Image.open('/content/drive/My Drive/sgmoid_train_imgs/000c1434d8d7.png')

imgs_path =[]
parent = '/content/drive/My Drive/sgmoid_train_imgs/'
for i in train['id_code']:
  path = parent + i + '.png'
  imgs_path.append(path)
imgs_path = pd.Series(imgs_path)
print(len(imgs_path))

train_data = pd.concat([train,imgs_path],axis=1)
print(train_data.shape)
train_data.rename(columns= {0: 'file'},inplace=True)
train_data.head()

train_data['diagnosis'] = train_data['diagnosis'].astype(str)

import matplotlib.pyplot as plt
import seaborn as sns
import torchvision.models as models
import tensorflow as tf
import keras 
from PIL import Image
import torch.utils.data as dl
import torch

# class Dataload(dl.Dataset):
#     def __init__(self):
#         self.len = train.shape[0]
#         self.X_data = train_images
#         self.Y_data = train.iloc[:,-1]
#     def __getitem__(self,index):
#         img, tar = self.X_data[index], self.Y_data[index]
#         img = Image.open(img)
#         arr = np.array(img.resize((512,512)))
#         arr = arr/255.0
#         return arr,tar
    
#     def __len__(self):
#         return self.len
    
# dataset = Dataload()
# train_loader = dl.DataLoader(dataset=dataset,batch_size=16,shuffle=True,num_workers=15)

# pixels=[]
# for i,data in enumerate(train_loader,0):
#     img,tar = data
#     print((img.shape),len(tar))
#     print(img[0].shape)
#     break

import torch
import torch.nn as nn
import torchvision.models as models
import torch.optim as optim

from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.models import Model
from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,Dropout
from keras.preprocessing import image
from keras.optimizers import Adam
import keras.backend as K

datagen = ImageDataGenerator(horizontal_flip=True,vertical_flip=False,rescale=1/255.0)

train_generator=datagen.flow_from_dataframe(dataframe=train_data,directory="/content/drive/My Drive/sgmoid_train_imgs/",x_col="file",y_col="diagnosis",
                                            subset="training",batch_size=64,seed=42,shuffle=True,
                                            class_mode="categorical",target_size=(128,128))

valid_generator=datagen.flow_from_dataframe(dataframe=train_data,directory="/content/drive/My Drive/sgmoid_train_imgs/",x_col="file",y_col="diagnosis",
                                            subset="validation",batch_size=64,seed=42,shuffle=True,
                                            class_mode="categorical",target_size=(128,128))

# train_datagen= ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.1, horizontal_flip= True)
# valid_datagen= ImageDataGenerator(rescale=1./255)
# size=(512,512)      
# in_shape=(512,512,3)
# train_set= train_datagen.flow_from_directory('/content/drive/My Drive/sgmoid_train_imgs/', 
#                                              target_size=size, batch_size=50, class_mode='categorical', 
#                                              shuffle=True, seed=20)
# valid_set= valid_datagen.flow_from_directory('/content/drive/My Drive/sgmoid_train_imgs/', 
#                                              target_size=size, batch_size=50, class_mode='categorical', 
#                                              shuffle=False)

import warnings
warnings.filterwarnings('ignore')

resnet = keras.applications.resnet_v2.ResNet152V2(include_top=False, weights='imagenet', input_tensor=None, input_shape=(128,128,3), pooling=None, classes=5)

print(resnet)

# resnet = keras.applications.resnet.ResNet152(include_top=False, weights='imagenet', input_shape=(512,512,3))

model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])
step_size_train=train_generator.n//train_generator.batch_size
# step_size_valid=valid_set.n//valid_set.batch_size
model.fit_generator(train_generator, steps_per_epoch=step_size_train, epochs=10)
model.save('resnet152V2.h5')

import pickle

from keras.models import load_model
model=load_model('/content/resnet152V2.h5')

model.summary()

"""###Testing Images"""

# !unzip /content/drive/My\ Drive/Sgmoid1611/test_images.zip

test = pd.read_csv('/content/drive/My Drive/Sgmoid1611/test.csv')
print(test.shape)
test.head()

import glob
total_images=0
test_images = []
test_images +=  glob.glob('/content/drive/My Drive/sgmoid_test_imgs/*.png')
total_images = len(test_images)
print(total_images)

# import shutil
# import os
# destination = "/content/drive/My Drive/sgmoid_test_imgs/"
# for i in test_images:
#   path = destination + i
#   dest = shutil.copyfile(i,path)

test_images[0]

imgs_path =[]
parent = '/content/drive/My Drive/sgmoid_test_imgs/'
for i in test['id_code']:
  path = parent + i + '.png'
  imgs_path.append(path)
imgs_path = pd.Series(imgs_path)
print(len(imgs_path))

test_data = pd.concat([test,imgs_path],axis=1)
print(test_data.shape)
test_data.rename(columns= {0: 'file'},inplace=True)
test_data.head()

test_pred=[]

from keras.preprocessing.image import img_to_array
from keras.preprocessing.image import load_img

for i in test_data['file']:
  image = load_img(i,target_size=(128,128))
  image = img_to_array(image)
  image = image.reshape(1,128,128,3)
  pred = model.predict(image)
  test_pred.append(pred)
len(test_pred)

test_labels = []
for pred in test_pred:
  clas = np.argmax(pred)
  test_labels.append(clas)
len(test_labels)

testing = pd.Series(test_labels)
testing.head()

testing_id = test_data['id_code']
testing_id.head()

submission = pd.concat([testing_id,testing],axis=1)
submission.rename(columns={0:'diagnosis'},inplace=True)
print(submission.shape)
submission.head()

submission.to_csv('/content/sample_data/Submission_resnet.csv',index=False)

sample = pd.read_csv('/content/sample_data/Submission_resnet.csv')
sample.head()

print(test_labels)

def Image_numpy(img_file):
  from keras.preprocessing.image import img_to_array
  from keras.preprocessing.image import load_img
  image = load_img(img_file,target_size=(128,128))
  image = img_to_array(image)
  image = image.reshape(1,128,128,3)
  return image

pickle.dump(model, open('resnet152.pkl','wb'))

img_np = Image_numpy(imgs_path[0])
np.argmax(model.predict(img_np))

pickle.dump(model, open('resnet152.pkl','wb'))

res_model = pickle.load(open('resnet152.pkl','rb'))
print(np.argmax(res_model.predict(Image_numpy(imgs_path[0]))))

